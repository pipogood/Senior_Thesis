{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CWT & STFT + CNN Model with Unicorn Hybrid Black dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting EDF parameters from C:\\git\\Senior_Thesis\\DataSet\\Convert_data\\hand0_new.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 279249  =      0.000 ...  1116.996 secs...\n",
      "Extracting EDF parameters from C:\\git\\Senior_Thesis\\DataSet\\Convert_data\\hand1_new.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 279249  =      0.000 ...  1116.996 secs...\n",
      "Extracting EDF parameters from C:\\git\\Senior_Thesis\\DataSet\\Convert_data\\hand2_new.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 279249  =      0.000 ...  1116.996 secs...\n",
      "Extracting EDF parameters from C:\\git\\Senior_Thesis\\DataSet\\Convert_data\\hand3_new.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 279249  =      0.000 ...  1116.996 secs...\n",
      "Extracting EDF parameters from C:\\git\\Senior_Thesis\\DataSet\\Convert_data\\hand4_new.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 279249  =      0.000 ...  1116.996 secs...\n",
      "Extracting EDF parameters from C:\\git\\Senior_Thesis\\DataSet\\Convert_data\\hand5_new.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 279249  =      0.000 ...  1116.996 secs...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"table table-hover table-striped table-sm table-responsive small\">\n",
       "    <tr>\n",
       "        <th>Measurement date</th>\n",
       "        \n",
       "        <td>October 01, 2023  19:53:00 GMT</td>\n",
       "        \n",
       "    </tr>\n",
       "    <tr>\n",
       "        <th>Experimenter</th>\n",
       "        \n",
       "        <td>Unknown</td>\n",
       "        \n",
       "    </tr>\n",
       "        <th>Participant</th>\n",
       "        \n",
       "        <td>Unknown</td>\n",
       "        \n",
       "    </tr>\n",
       "    <tr>\n",
       "        <th>Digitized points</th>\n",
       "        \n",
       "        <td>11 points</td>\n",
       "        \n",
       "    </tr>\n",
       "    <tr>\n",
       "        <th>Good channels</th>\n",
       "        <td>8 EEG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <th>Bad channels</th>\n",
       "        <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <th>EOG channels</th>\n",
       "        <td>Not available</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <th>ECG channels</th>\n",
       "        <td>Not available</td>\n",
       "    \n",
       "    <tr>\n",
       "        <th>Sampling frequency</th>\n",
       "        <td>250.00 Hz</td>\n",
       "    </tr>\n",
       "    \n",
       "    \n",
       "    <tr>\n",
       "        <th>Highpass</th>\n",
       "        <td>0.00 Hz</td>\n",
       "    </tr>\n",
       "    \n",
       "    \n",
       "    <tr>\n",
       "        <th>Lowpass</th>\n",
       "        <td>125.00 Hz</td>\n",
       "    </tr>\n",
       "    \n",
       "    \n",
       "    \n",
       "    <tr>\n",
       "        <th>Filenames</th>\n",
       "        <td>hand3_new.edf</td>\n",
       "    </tr>\n",
       "    \n",
       "    <tr>\n",
       "        <th>Duration</th>\n",
       "        <td>00:18:37 (HH:MM:SS)</td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<RawEDF | hand3_new.edf, 8 x 279250 (1117.0 s), ~17.1 MB, data loaded>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mne\n",
    "import numpy as np\n",
    "from mne.datasets import eegbci\n",
    "import matplotlib.pyplot as plt\n",
    "from mne.channels import make_standard_montage\n",
    "\n",
    "raw_each = [0] * 6\n",
    "\n",
    "for i in range(0,6):\n",
    "    raw_each[i] = mne.io.read_raw_edf(\"C:\\git\\Senior_Thesis\\DataSet\\Convert_data\\hand\"+ str(i) +\"_new.edf\",preload = True)\n",
    "\n",
    "# raw_edf = mne.concatenate_raws(raw_each)\n",
    "# raw_edf = mne.concatenate_raws([raw_each[1], raw_each[3], raw_each[4]])\n",
    "\n",
    "raw_edf = mne.concatenate_raws([raw_each[3]])\n",
    "\n",
    "# raw_edf = mne.io.read_raw_edf(\"C:\\git\\Senior_Thesis\\DataSet\\Convert_data\\MI_execution.edf\", preload=True)\n",
    "\n",
    "eegbci.standardize(raw_edf)  # set channel names\n",
    "montage = make_standard_montage(\"standard_1005\")\n",
    "raw_edf.set_montage(montage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw_edf.plot(scalings = 100, start= 45,  duration=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Lambda, BatchNormalization, Activation, Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import History,ModelCheckpoint\n",
    "from keras.utils import plot_model\n",
    "history = History()\n",
    "import pywt\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from mne.decoding import CSP\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.model_selection import ShuffleSplit,StratifiedKFold ,cross_val_score, cross_val_predict\n",
    "from ssqueezepy import ssq_cwt, ssq_stft\n",
    "import pickle\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 1 - 30 Hz\n",
      "\n",
      "IIR filter parameters\n",
      "---------------------\n",
      "Butterworth bandpass zero-phase (two-pass forward and reverse) non-causal filter:\n",
      "- Filter order 20 (effective, after forward-backward)\n",
      "- Cutoffs at 1.00, 30.00 Hz: -6.02, -6.02 dB\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EEG channel type selected for re-referencing\n",
      "Applying average reference.\n",
      "Applying a custom ('EEG',) reference.\n",
      "Used Annotations descriptions: ['OVTK_GDF_Cross_On_Screen', 'OVTK_GDF_End_Of_Session', 'OVTK_GDF_End_Of_Trial', 'OVTK_GDF_Feedback_Continuous', 'OVTK_GDF_Incorrect', 'OVTK_GDF_Left', 'OVTK_GDF_Right', 'OVTK_GDF_Start_Of_Trial', 'OVTK_GDF_Tongue', 'OVTK_GDF_Up', 'OVTK_StimulationId_BaselineStart', 'OVTK_StimulationId_BaselineStop', 'OVTK_StimulationId_Beep', 'OVTK_StimulationId_Train']\n",
      "Multiple event values for single event times found. Keeping the first occurrence and dropping all others.\n",
      "Not setting metadata\n",
      "605 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 605 events and 1501 original time points ...\n",
      "1 bad epochs dropped\n"
     ]
    }
   ],
   "source": [
    "eeg1 = raw_edf.copy().filter(l_freq=1.0, h_freq=30.0, method = 'iir', iir_params= {\"order\": 5, \"ftype\":'butter'})\n",
    "# eeg1 = raw_edf.copy().filter(l_freq=0.075, h_freq=3.0, method = 'fir')\n",
    "eeg1 = eeg1.copy().set_eeg_reference(ref_channels=\"average\")\n",
    "\n",
    "eeg1= eeg1.pick([\"Fz\",\"C3\", \"Cz\", \"C4\",\"Pz\",'PO7','PO8'])\n",
    "# eeg1= eeg1.pick_channels([\"C3\", \"Cz\", \"C4\"])s\n",
    "events, event_dict = mne.events_from_annotations(eeg1)\n",
    "combine_epochs = mne.Epochs(eeg1, events, \n",
    "        tmin=-2.0,     # init timestamp of epoch (0 means trigger timestamp same as event start)\n",
    "        tmax=4.0,    # final timestamp (10 means set epoch duration 10 second)\n",
    "        event_id=event_dict,\n",
    "        preload = True,\n",
    "        event_repeated='drop',\n",
    "        baseline=(-2, 0)\n",
    "    )\n",
    "\n",
    "combine_epochs = combine_epochs.copy().crop(tmin=0.0, tmax=2.0)\n",
    "component_num = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(604, 7, 501)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(combine_epochs.get_data())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_left = combine_epochs['OVTK_GDF_Left'].to_data_frame()\n",
    "df_right = combine_epochs['OVTK_GDF_Right'].to_data_frame()\n",
    "df_up = combine_epochs['OVTK_GDF_Up'].to_data_frame()\n",
    "df_tongue = combine_epochs['OVTK_GDF_Tongue'].to_data_frame()\n",
    "df_left =df_left.iloc[:, -7:]\n",
    "df_right =df_right.iloc[:, -7:]\n",
    "df_up =df_up.iloc[:, -7:]\n",
    "df_tongue =df_tongue.iloc[:, -7:]\n",
    "\n",
    "# # Plot histograms for all columns in the DataFrame using Seaborn\n",
    "# fig, ax = plt.subplots(1, len(df_left.columns),sharey=True, figsize=(10 * len(df_left.columns), 10))\n",
    "# for i, column in enumerate(df_left.columns):\n",
    "#     ax[i] = sns.histplot(df_left[column], kde=False, ax=ax[i], color= 'orange')\n",
    "#     ax[i] = sns.histplot(df_right[column], kde=False, ax=ax[i], color= 'blue')\n",
    "#     ax[i] = sns.histplot(df_up[column], kde=False, ax=ax[i], color= 'green')\n",
    "#     ax[i] = sns.histplot(df_tongue[column], kde=False, ax=ax[i], color= 'pink')\n",
    "#     ax[i].set_xlabel(column,  fontsize=30)\n",
    "#     ax[i].set_ylabel('Frequency',  fontsize=30)\n",
    "#     ax[i].tick_params(axis='both', labelsize=30)\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Example DataFrame (replace this with your actual DataFrame)\n",
    "# data = {\n",
    "#     'column1': [1, 2, 3, 4, 5],\n",
    "#     'column2': [2, 3, 4, 5, 6],\n",
    "#     'column3': [3, 4, 5, 6, 7],\n",
    "#     'column4': [4, 5, 6, 7, 8],\n",
    "#     'column5': [5, 6, 7, 8, 9]\n",
    "# }\n",
    "# df = pd.DataFrame(data)\n",
    "\n",
    "# # Select the last three columns\n",
    "# last_three_columns = df.iloc[:, -3:]\n",
    "\n",
    "# # Plot histograms for the last three columns using Seaborn in a horizontal line\n",
    "# fig, ax = plt.subplots(1, len(last_three_columns.columns), sharey=True, figsize=(6 * len(last_three_columns.columns), 6))\n",
    "# for i, column in enumerate(last_three_columns.columns):\n",
    "#     sns.histplot(last_three_columns[column], kde=False, bins=10, ax=ax[i])\n",
    "#     ax[i].set_xlabel(column)\n",
    "#     ax[i].set_ylabel('Frequency')\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FastICA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from mne.preprocessing import ICA\n",
    "# ica = ICA(n_components=component_num, max_iter=\"auto\", random_state=97)\n",
    "# ica.fit(combine_epochs)\n",
    "# # ica.plot_sources(combine_epochs)\n",
    "\n",
    "# with open('trained_ica_model.pkl', 'wb') as file:\n",
    "#     pickle.dump(ica, file)\n",
    "\n",
    "##################### load FastICA ########################\n",
    "\n",
    "# with open('trained_ica_model.pkl', 'rb') as file:\n",
    "#     trained_ica = pickle.load(file)\n",
    "\n",
    "# combine_epochs = trained_ica.get_sources(combine_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TRY FBCSP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Prepare Labels and Train data\n",
    "# labels = combine_epochs['OVTK_GDF_Left','OVTK_GDF_Right','OVTK_GDF_Up','OVTK_GDF_Tongue'].events[:,2]\n",
    "# for i in range(0,len(labels)):\n",
    "#     if labels[i] > 7:\n",
    "#         labels[i] = labels[i] - 1\n",
    "#     # if labels[i] < 7:\n",
    "#     #     labels[i] = labels[i] + 1\n",
    "\n",
    "# low_fre = [6, 10, 14, 18, 22, 26]\n",
    "# high_fre = [12, 16, 20, 24, 28, 32]\n",
    "\n",
    "# shape = combine_epochs['OVTK_GDF_Left','OVTK_GDF_Right','OVTK_GDF_Up','OVTK_GDF_Tongue'].get_data().shape\n",
    "\n",
    "# train_data = np.ndarray(shape= (6,shape[0],shape[1],shape[2]))\n",
    "# new_data = np.ndarray(shape= (6,shape[0],shape[1],shape[2]))\n",
    "\n",
    "# for i in range(0,6):\n",
    "#     combine_epochs = combine_epochs.filter(l_freq=low_fre[i], h_freq=high_fre[i], method = 'iir', iir_params= {\"order\": 5, \"ftype\":'butter'})\n",
    "#     train_data[i] = combine_epochs['OVTK_GDF_Left','OVTK_GDF_Right','OVTK_GDF_Up','OVTK_GDF_Tongue'].get_data()\n",
    "    \n",
    "#     # csp = CSP(n_components=component_num, reg=None, log=None, transform_into='csp_space')\n",
    "#     # csp.fit(train_data[i], labels)\n",
    "\n",
    "#     # with open('trained_FBCSP_model'+str(i)+'.pkl', 'wb') as file:\n",
    "#     #     pickle.dump(csp, file)\n",
    "\n",
    "# for j in range(0,6):\n",
    "#     with open('trained_FBCSP_model'+str(j)+'.pkl', 'rb') as file:\n",
    "#         trained_csp = pickle.load(file)\n",
    "\n",
    "#     new_data[j] = trained_csp.transform(train_data[j])\n",
    "\n",
    "# new_data = np.concatenate(new_data, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Labels and Train data\n",
    "labels = combine_epochs['OVTK_GDF_Left','OVTK_GDF_Right','OVTK_GDF_Up','OVTK_GDF_Tongue'].events[:,2]\n",
    "for i in range(0,len(labels)):\n",
    "    if labels[i] > 7:\n",
    "        labels[i] = labels[i] - 1\n",
    "    # if labels[i] < 7:\n",
    "    #     labels[i] = labels[i] + 1\n",
    "train_data = combine_epochs['OVTK_GDF_Left','OVTK_GDF_Right','OVTK_GDF_Up','OVTK_GDF_Tongue'].get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'OVTK_GDF_Cross_On_Screen': 1,\n",
       " 'OVTK_GDF_End_Of_Session': 2,\n",
       " 'OVTK_GDF_End_Of_Trial': 3,\n",
       " 'OVTK_GDF_Feedback_Continuous': 4,\n",
       " 'OVTK_GDF_Left': 6,\n",
       " 'OVTK_GDF_Right': 7,\n",
       " 'OVTK_GDF_Start_Of_Trial': 8,\n",
       " 'OVTK_GDF_Tongue': 9,\n",
       " 'OVTK_GDF_Up': 10,\n",
       " 'OVTK_StimulationId_BaselineStart': 11,\n",
       " 'OVTK_StimulationId_BaselineStop': 12,\n",
       " 'OVTK_StimulationId_Beep': 13,\n",
       " 'OVTK_StimulationId_Train': 14}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combine_epochs.event_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([8, 9, 7, 6, 8, 9, 7, 8, 8, 9, 9, 8, 6, 6, 7, 8, 6, 9, 7, 6, 8, 9,\n",
       "       7, 8, 8, 9, 7, 6, 7, 9, 7, 6, 6, 9, 7, 6, 9, 9, 7, 6, 8, 6, 7, 6,\n",
       "       8, 9, 6, 6, 9, 9, 7, 6, 6, 8, 7, 7, 8, 9, 7, 6, 8, 7, 7, 6, 8, 9,\n",
       "       7, 7, 8, 8, 7, 6, 8, 9, 7, 6, 8, 9, 7, 6, 9, 9, 6, 6, 8, 9, 7, 6,\n",
       "       9, 9, 7, 6, 8, 9, 7, 7, 6, 9, 9, 9, 8, 8, 7, 8, 8, 9, 7, 6, 8, 8,\n",
       "       7, 6, 8, 9, 9, 6, 8, 8, 7, 6])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train with CSP+LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lda = LinearDiscriminantAnalysis()\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "# csp2 = CSP(n_components=7, reg=None, log=None)\n",
    "# clf = Pipeline([(\"CSP\", csp2), (\"LDA\", lda)])\n",
    "# scores = cross_val_score(clf, train_data, labels, cv=10, n_jobs=None)\n",
    "# CSP_predicted = cross_val_predict(clf, train_data, labels, cv=10)\n",
    "\n",
    "# conf_matrix = confusion_matrix(labels, CSP_predicted)\n",
    "# print(conf_matrix)\n",
    "# print('Accuracy',np.mean(scores))\n",
    "\n",
    "# import seaborn as sns\n",
    "# class_names = ['Left hand', 'Right hand',  'Non-imagine','Feet']\n",
    "\n",
    "# sns.heatmap(conf_matrix, \n",
    "#             annot=True,\n",
    "#             fmt='g', \n",
    "#             xticklabels=class_names,\n",
    "#             yticklabels=class_names)\n",
    "# plt.ylabel('Prediction',fontsize=13)\n",
    "# plt.xlabel('Actual',fontsize=13)\n",
    "# plt.title('Confusion Matrix CSP+LDA',fontsize=17)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSP -> CWT -> CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train csp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# csp = CSP(n_components=component_num, reg=None, log=None, transform_into='csp_space')\n",
    "# csp.fit(train_data, labels)\n",
    "\n",
    "# with open('trained_csp_model.pkl', 'wb') as file:\n",
    "#     pickle.dump(csp, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load csp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120, 7, 501)\n"
     ]
    }
   ],
   "source": [
    "with open('trained_csp_model.pkl', 'rb') as file:\n",
    "    trained_csp = pickle.load(file)\n",
    "\n",
    "new_data = trained_csp.transform(train_data)\n",
    "print(new_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "try csp+lda trined model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA only classification scores 0.7666666666666667\n",
      "[[20  6  1  3]\n",
      " [ 6 17  2  5]\n",
      " [ 1  1 26  2]\n",
      " [ 3  5  1 21]]\n"
     ]
    }
   ],
   "source": [
    "# csp2 = CSP(n_components=7, reg=None, log=None)\n",
    "# csp2.fit(train_data, labels)\n",
    "# with open('trained_csp2_model.pkl', 'wb') as file:\n",
    "#     pickle.dump(csp2, file)\n",
    "\n",
    "with open('trained_csp2_model.pkl', 'rb') as file:\n",
    "    trained_csp2 = pickle.load(file)\n",
    "new_data2 = trained_csp2.transform(train_data)\n",
    "\n",
    "# lda2 = LinearDiscriminantAnalysis()\n",
    "# lda2.fit(new_data2, labels)\n",
    "# with open('trained_lda_model.pkl', 'wb') as file:\n",
    "#     pickle.dump(lda2, file)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "with open('trained_lda_model.pkl', 'rb') as file:\n",
    "    trained_lda = pickle.load(file)\n",
    "score = trained_lda.score(new_data2, labels)\n",
    "lda_predicted = cross_val_predict(trained_lda, new_data2, labels)\n",
    "conf_matrix = confusion_matrix(labels, lda_predicted)\n",
    "print(\"LDA only classification scores\", score)\n",
    "print(conf_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120, 7, 501)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(new_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot topographic patterns of components. The patterns explain how the measured data was generated from the neural sources (a.k.a. the forward model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trained_csp.plot_patterns(combine_epochs.info, ch_type=\"eeg\", units=\"Patterns (AU)\", size=1.5, vlim=(-5e6, 5e6))\n",
    "# print('...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selected_events = combine_epochs['OVTK_GDF_Left','OVTK_GDF_Right','OVTK_GDF_Up','OVTK_GDF_Tongue']\n",
    "# csp_epochs = mne.EpochsArray(new_data, selected_events.info, events=selected_events.events, event_id= selected_events.event_id)\n",
    "# channel_mapping = {\n",
    "#     'Fz': 'csp1',\n",
    "#     'C3': 'csp2',\n",
    "#     'Cz': 'csp3',\n",
    "#     'C4': 'csp4',\n",
    "#     'Pz': 'csp5',\n",
    "#     'PO7': 'csp6',\n",
    "#     'PO8': 'csp7'\n",
    "# }\n",
    "# csp_epochs.rename_channels(channel_mapping)\n",
    "\n",
    "# csp_left_df = csp_epochs['OVTK_GDF_Left'].to_data_frame()\n",
    "# csp_right_df = csp_epochs['OVTK_GDF_Right'].to_data_frame()\n",
    "# csp_up_df = csp_epochs['OVTK_GDF_Up'].to_data_frame()\n",
    "# csp_tongue_df = csp_epochs['OVTK_GDF_Tongue'].to_data_frame()\n",
    "\n",
    "# csp_left_df =csp_left_df.iloc[:, -7:]\n",
    "# csp_right_df =csp_right_df.iloc[:, -7:]\n",
    "# csp_up_df =csp_up_df.iloc[:, -7:]\n",
    "# csp_tongue_df =csp_tongue_df.iloc[:, -7:]\n",
    "\n",
    "# # Plot histograms for all columns in the DataFrame using Seaborn\n",
    "# fig, ax = plt.subplots(1, len(csp_left_df.columns),sharey=True, figsize=(10 * len(csp_left_df.columns), 10))\n",
    "# for i, column in enumerate(csp_left_df.columns):\n",
    "#     ax[i] = sns.histplot(csp_left_df[column], kde=False, ax=ax[i], color= 'orange')\n",
    "#     ax[i] = sns.histplot(csp_right_df[column], kde=False, ax=ax[i], color= 'blue')\n",
    "#     ax[i] = sns.histplot(csp_up_df[column], kde=False, ax=ax[i], color= 'green')\n",
    "#     ax[i] = sns.histplot(csp_tongue_df[column], kde=False, ax=ax[i], color= 'pink')\n",
    "#     ax[i].set_xlabel(column,  fontsize=30)\n",
    "#     # ax[i].set_xlim(-1e7, 1e7)\n",
    "#     ax[i].set_ylabel('Frequency',  fontsize=30)\n",
    "#     ax[i].tick_params(axis='both', labelsize=30)\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# info = combine_epochs['OVTK_GDF_Left','OVTK_GDF_Right','OVTK_GDF_Up','OVTK_GDF_Tongue'].info\n",
    "# event = combine_epochs['OVTK_GDF_Left','OVTK_GDF_Right','OVTK_GDF_Up','OVTK_GDF_Tongue'].events\n",
    "# event_ids = combine_epochs['OVTK_GDF_Left','OVTK_GDF_Right','OVTK_GDF_Up','OVTK_GDF_Tongue'].event_id\n",
    "\n",
    "# forFFT = mne.EpochsArray(new_data, info, events=event, event_id=event_ids)\n",
    "\n",
    "# train_FFT = forFFT.compute_psd(method='welch',fmin = 1, fmax=30)\n",
    "# train_FFT.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from ssqueezepy import ssq_cwt, ssq_stft\n",
    "\n",
    "# n_fft = 256  # Number of DFT points\n",
    "# hop_length = int(n_fft * 0.03)  # 97% overlapping\n",
    "# win_length = int(n_fft * 0.5)   # 0.5 seconds window length\n",
    "# window = 'hamming' \n",
    "# train_size = len(labels)\n",
    "# train_data_stft = np.ndarray(shape=(train_size,component_num, 129, 72))\n",
    "\n",
    "\n",
    "# # _,coeff, *_ = ssq_stft(new_data[0,:], n_fft=n_fft, hop_len=hop_length, win_len=win_length, window=window)\n",
    "# for i in range(0,train_size):\n",
    "#     _,coeff, *_ = ssq_stft(new_data[i,:], n_fft=n_fft, hop_len=hop_length, win_len=win_length, window=window)\n",
    "#     train_data_stft[i, :, :, :] = abs(coeff)\n",
    "\n",
    "# print(np.shape(train_data_stft))\n",
    "\n",
    "# # Stack array and convert to image\n",
    "# train_stft_stack = np.ndarray(shape=(train_size, train_data_stft.shape[2],train_data_stft.shape[3]*component_num))\n",
    "\n",
    "# for jj in range(0,train_data_stft.shape[0]):\n",
    "#     train_stft_stack[jj] = np.concatenate(train_data_stft[jj], axis = 1)\n",
    "#     # train_stft_stack[jj] = np.vstack((train_data_stft[jj,0,:,:], train_data_stft[jj,1,:,:], train_data_stft[jj,2,:,:], train_data_stft[jj,3,:,:], train_data_stft[jj,4,:,:], train_data_stft[jj,5,:,:], train_data_stft[jj,6,:,:]))\n",
    "\n",
    "# print(np.shape(train_stft_stack))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CWT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120, 501, 7)\n",
      "0\n",
      "40\n",
      "80\n",
      "(120, 30, 501, 7)\n",
      "(120, 210, 501)\n"
     ]
    }
   ],
   "source": [
    "FB = 1\n",
    "train_cwt = np.ndarray(shape=(new_data.shape[0], new_data.shape[2], component_num*FB))\n",
    "for jj in range(0, new_data.shape[0]):\n",
    "    train_cwt[jj] = new_data[jj].T\n",
    "print(np.shape(train_cwt))\n",
    "\n",
    "scales = range(1,31)\n",
    "\n",
    "waveletname = 'morl'\n",
    "train_size = len(labels)\n",
    "train_data_cwt = np.ndarray(shape=(train_size, len(scales), new_data.shape[2], component_num*FB))\n",
    "\n",
    "for ii in range(0,train_size):\n",
    "    if ii % 40 == 0:\n",
    "        print(ii)\n",
    "    for jj in range(0,component_num*FB):\n",
    "        signal = train_cwt[ii, :, jj]\n",
    "        coeff, _ = pywt.cwt(signal, scales, waveletname, 1)\n",
    "        coeff_ = coeff[:,:new_data.shape[2]]  #crop 227 sample for each channel\n",
    "        train_data_cwt[ii, :, :, jj] = abs(coeff_)\n",
    "print(np.shape(train_data_cwt))\n",
    "\n",
    "train_cwt_stack = np.ndarray(shape=(train_size, len(scales)*component_num*FB, new_data.shape[2]))\n",
    "\n",
    "for jj in range(0,train_data_cwt.shape[0]):\n",
    "    # train_cwt_stack[jj] = np.concatenate(train_data_cwt[jj], axis = 1)\n",
    "    train_cwt_stack[jj] = np.vstack((train_data_cwt[jj,:,:,0], train_data_cwt[jj,:,:,1], train_data_cwt[jj,:,:,2], train_data_cwt[jj,:,:,3], train_data_cwt[jj,:,:,4], train_data_cwt[jj,:,:,5], train_data_cwt[jj,:,:,6]))\n",
    "\n",
    "print(np.shape(train_cwt_stack))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import StandardScaler\n",
    "# scaler = StandardScaler()\n",
    "# reshaped_data = train_stft_stack.reshape((train_stft_stack.shape[0], -1))\n",
    "# scaled_data = scaler.fit_transform(reshaped_data)\n",
    "# scaled_data_3d = scaled_data.reshape(train_stft_stack.shape)\n",
    "# print(scaled_data_3d.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(84, 210, 501) (36, 210, 501) (84,) (36,)\n"
     ]
    }
   ],
   "source": [
    "# train_cwt_stack = scaled_data_3d\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(train_cwt_stack, labels, test_size=0.3, random_state=42)\n",
    "\n",
    "num_classes = 4\n",
    "\n",
    "batch_size = 1\n",
    "epochs = 30\n",
    "print(np.shape(x_train), np.shape(x_test), np.shape(y_train), np.shape(y_test))\n",
    "\n",
    "y_train = keras.utils.to_categorical(y_train-6, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test-6, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Purposed CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the CNN model\n",
    "# model = Sequential()\n",
    "\n",
    "# # Convolutional Layer 1 with 32 filters of kernel size 3x3\n",
    "# model.add(Conv2D(32, (3, 3), input_shape=(train_cwt_stack.shape[1], train_cwt_stack.shape[2], 1), activation='relu'))\n",
    "# # model.add(BatchNormalization())\n",
    "# # Max-Pooling Layer 1 with pool size 2x2\n",
    "# model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# # Convolutional Layer 2 with 64 filters of kernel size 3x3\n",
    "# model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "# # model.add(BatchNormalization())\n",
    "# # model.add(Dropout(0.5)) \n",
    "\n",
    "# # Max-Pooling Layer 2 with pool size 2x2\n",
    "# model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# # Flatten the output from the previous layer\n",
    "# model.add(Flatten())\n",
    "\n",
    "# # Output Layer with the desired number of output nodes (adjust as needed)\n",
    "# model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# model.compile(loss=keras.losses.categorical_crossentropy, \n",
    "#               optimizer=keras.optimizers.Adam(learning_rate= 0.0001), \n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "# model.summary()\n",
    "# plot_model(model,to_file='model_plot.png', show_shapes=True, show_layer_names=True)\n",
    "# checkpoint = ModelCheckpoint(\"CNN_try_model_weights.h5\", monitor='val_accuracy', save_best_only=True, mode='max', verbose=1)\n",
    "\n",
    "# model.fit(x_train, y_train, batch_size=batch_size, \n",
    "#           epochs=epochs, verbose=1, \n",
    "#           validation_data=(x_test, y_test), \n",
    "#           callbacks=[history, checkpoint])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TRY MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the CNN model\n",
    "# model = Sequential()\n",
    "\n",
    "# # Convolutional Layer 1 with 32 filters of kernel size 3x3\n",
    "# model.add(Conv2D(32, (3, 3), input_shape=(train_cwt_stack.shape[1], train_cwt_stack.shape[2], 1), activation='relu'))\n",
    "# # model.add(BatchNormalization())\n",
    "# # Max-Pooling Layer 1 with pool size 2x2\n",
    "# model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# # Flatten the output from the previous layer\n",
    "# model.add(Flatten())\n",
    "# model.add(Dense(128))\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(Dropout(0.5))\n",
    "\n",
    "# # Output Layer with the desired number of output nodes (adjust as needed)\n",
    "# model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# model.compile(loss=keras.losses.categorical_crossentropy, \n",
    "#               optimizer=keras.optimizers.Adam(learning_rate= 0.0001), \n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "# model.summary()\n",
    "# plot_model(model,to_file='model_plot.png', show_shapes=True, show_layer_names=True)\n",
    "# checkpoint = ModelCheckpoint(\"CNN_try_model_weights.h5\", monitor='val_accuracy', save_best_only=True, mode='max', verbose=1)\n",
    "\n",
    "# model.fit(x_train, y_train, batch_size=batch_size, \n",
    "#           epochs=epochs, verbose=1, \n",
    "#           validation_data=(x_test, y_test), \n",
    "#           callbacks=[history, checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from statistics import mean \n",
    "# fig, axarr = plt.subplots(figsize=(12,6), ncols=2)\n",
    "# axarr[0].plot(range(1, epochs+1), history.history['accuracy'], label='Train')\n",
    "# axarr[0].plot(range(1, epochs+1), history.history['val_accuracy'], label='Test')\n",
    "# axarr[0].set_xlabel('Number of Epochs', fontsize=18)\n",
    "# axarr[0].set_ylabel('Accuracy', fontsize=18)\n",
    "# axarr[0].title.set_text('Max Acc = {}'.format(max(history.history['val_accuracy'])))\n",
    "# axarr[0].set_ylim([0,1.1])\n",
    "# axarr[1].plot(range(1, epochs+1), history.history['loss'], label='Train')\n",
    "# axarr[1].plot(range(1, epochs+1), history.history['val_loss'], label='Test')\n",
    "# axarr[1].set_xlabel('Number of Epochs', fontsize=18)\n",
    "# axarr[1].set_ylabel('Loss', fontsize=18)\n",
    "# axarr[1].set_ylim([0,3.5])\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\pipo_\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\engine\\training.py\", line 1972, in test_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\pipo_\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\engine\\training.py\", line 1956, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\pipo_\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\engine\\training.py\", line 1944, in run_step  **\n        outputs = model.test_step(data)\n    File \"C:\\Users\\pipo_\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\engine\\training.py\", line 1850, in test_step\n        y_pred = self(x, training=False)\n    File \"C:\\Users\\pipo_\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\pipo_\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\engine\\input_spec.py\", line 298, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"sequential_12\" is incompatible with the layer: expected shape=(None, 129, 504, 1), found shape=(None, 210, 501)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\git\\Senior_Thesis\\CWT_Unicorn.ipynb Cell 42\u001b[0m line \u001b[0;36m8\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/git/Senior_Thesis/CWT_Unicorn.ipynb#X56sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m pretrained_model \u001b[39m=\u001b[39m load_model(\u001b[39m\"\u001b[39m\u001b[39mCNN_try_model_weights.h5\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/git/Senior_Thesis/CWT_Unicorn.ipynb#X56sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m y_test \u001b[39m=\u001b[39m keras\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mto_categorical(labels \u001b[39m-\u001b[39m\u001b[39m6\u001b[39m , num_classes)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/git/Senior_Thesis/CWT_Unicorn.ipynb#X56sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m loss, accuracy \u001b[39m=\u001b[39m pretrained_model\u001b[39m.\u001b[39;49mevaluate(train_cwt_stack, y_test)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/git/Senior_Thesis/CWT_Unicorn.ipynb#X56sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m y_pred \u001b[39m=\u001b[39m pretrained_model\u001b[39m.\u001b[39mpredict(train_cwt_stack)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/git/Senior_Thesis/CWT_Unicorn.ipynb#X56sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m y_pred \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39margmax(y_pred, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m) \n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_file2o4vxlw_.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__test_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(step_function), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m), ag__\u001b[39m.\u001b[39mld(iterator)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"C:\\Users\\pipo_\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\engine\\training.py\", line 1972, in test_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\pipo_\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\engine\\training.py\", line 1956, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\pipo_\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\engine\\training.py\", line 1944, in run_step  **\n        outputs = model.test_step(data)\n    File \"C:\\Users\\pipo_\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\engine\\training.py\", line 1850, in test_step\n        y_pred = self(x, training=False)\n    File \"C:\\Users\\pipo_\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\pipo_\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\engine\\input_spec.py\", line 298, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"sequential_12\" is incompatible with the layer: expected shape=(None, 129, 504, 1), found shape=(None, 210, 501)\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "pretrained_model = load_model(\"CNN_try_model_weights.h5\")\n",
    "\n",
    "y_test = keras.utils.to_categorical(labels -6 , num_classes)\n",
    "\n",
    "loss, accuracy = pretrained_model.evaluate(train_cwt_stack, y_test)\n",
    "\n",
    "y_pred = pretrained_model.predict(train_cwt_stack)\n",
    "y_pred = np.argmax(y_pred, axis=1) \n",
    "y_true = np.argmax(y_test, axis=1)\n",
    "\n",
    "confusion_mat = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "class_names = ['Left hand', 'Right hand',  'Non-imagine', 'Feet']\n",
    "# class_names = ['imagine',  'Non-imagine',]\n",
    "\n",
    "sns.heatmap(confusion_mat, \n",
    "            annot=True,\n",
    "            fmt='d', \n",
    "            cmap=\"Blues\",\n",
    "            xticklabels=class_names,\n",
    "            yticklabels=class_names)\n",
    "plt.ylabel('Prediction',fontsize=13)\n",
    "plt.xlabel('Actual',fontsize=13)\n",
    "plt.title(\"Confusion Matrix: Accuracy = \"+ str(round(accuracy,3)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
